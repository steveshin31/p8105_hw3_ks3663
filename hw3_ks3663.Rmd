---
title: "p8105_hw3_ks3663"
author: "Kee-Young Shin"
date: "October 10, 2018"
output: github_document
---

## Problem 1
```{r}

library(tidyverse)
library(p8105.datasets)

data("brfss_smart2010") # import data

filtered_brfss = brfss_smart2010 %>%
  janitor::clean_names() %>%
  select(year, locationabbr, locationdesc, topic, response, sample_size, data_value) %>%
  filter(topic == "Overall Health") %>% # filter based on overall health
  mutate(response_f = factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"), ordered = TRUE)) # make column for factor variables 

levels(filtered_brfss$response_f)
class(filtered_brfss$response_f)
filtered_brfss

```
```{r}

filtered_brfss %>%
  filter(year == "2002") %>%
  select(locationabbr, locationdesc) %>%
  distinct() %>%
  group_by(locationabbr) %>%
  summarize(count = n()) %>%
  filter(count == 7)
# CT, FL, and NC were observed at 7 locations

locations_count = filtered_brfss %>%
  select(year, locationabbr, locationdesc) %>%
  distinct() %>% # distinct states
  group_by(year, locationabbr) %>%
  summarize(count = n()) # count number of locations
locations_count

ggplot(locations_count, aes(x = year, y = count, color = locationabbr)) + 
  geom_line()



```
```{r}

filtered_brfss %>%
  select(locationabbr, locationdesc, year, data_value, response) %>%
  filter(locationabbr == "NY", year %in% c(2002, 2006, 2010), 
         response == "Excellent") %>%
  group_by(year) %>%
  summarize(mean = mean(data_value), std = sd(data_value)) # show mean and std


filtered_brfss %>%
  filter(locationabbr == "NY", year %in% c(2002, 2006, 2010)) %>%
  select(locationabbr, locationdesc, year, response_f, sample_size) %>%
  spread(response_f, value = sample_size) %>%
  mutate(proportion_ex = Excellent / (Excellent + `Very good` + Good + Fair + Poor)) %>%
  group_by(year) %>%
  summarize(mean = mean(proportion_ex), std = sd(proportion_ex)) 
```

```{r}

average_proportions = filtered_brfss %>%
  select(locationabbr, locationdesc, year, response_f, sample_size) %>%
  spread(response_f, value = sample_size) %>%
  mutate(total_sample = (Excellent + `Very good` + Good + Fair + Poor)) %>%
  mutate(proportion_ex = Excellent / total_sample, proportion_vg = `Very good` / total_sample, proportion_g = Good / total_sample, proportion_f = Fair / total_sample, proportion_p = Poor / total_sample) %>% # create column for averages
  group_by(year, locationabbr) %>% # group by year and location
  summarize(mean_ex = mean(proportion_ex), mean_vg = mean(proportion_vg), mean_g = mean(proportion_g), mean_f = mean(proportion_f), mean_p = mean(proportion_p)) %>%
  ungroup() %>%
  gather(key = variable, value = value, 3:7) # reorder dataset 

average_proportions


ggplot(average_proportions, aes(x = year, y = value, color = locationabbr)) + 
  geom_line() + facet_wrap(~variable) # create plot of the averages 


```

## Problem 2
```{r}

data("instacart") # import data

instacart

dim(instacart)
```
There are 
```{r}

distinct(instacart, aisle_id) 
# There are 134 different aisles. 

instacart %>%
  group_by(aisle_id) %>% # group by aisle id
  summarize(n = n()) %>% # take count of number of times ordered 
  arrange(-n) # puts the aisle id in order from most ordered to least
# Most items are ordered from aisle 83 and aisle 24. 


aisle_orders = instacart %>%
  group_by(aisle_id) %>% # group by aisle id
  summarize(n = n())  # take count of number of times ordered 
  
aisle_orders

ggplot(aisle_orders, aes(x = aisle_id, y = n)) + geom_point()

```
```{r}

instacart %>%
  filter(aisle %in% c("dog food care", "baking ingredients", "packaged vegetables fruits")) %>% # filter by selected aisles 
  group_by(product_name, aisle) %>%
  summarize(count = n()) %>% # take count 
  ungroup() %>%
  group_by(aisle) %>%
  arrange(desc(count)) %>% # arrange count in descending order 
  slice(1) # show most ordered item 

```

```{r}

instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>% # calculate mean 
  spread(order_dow, mean_hour) # spread days into columns 


```


## Problem 3
```{r}
data("ny_noaa")
ny_noaa

weather_data = ny_noaa %>%
  janitor::clean_names() %>%
  mutate(tmax = as.numeric(tmax), tmin = as.numeric(tmin)) %>%
  separate(date, into = c("year", "month", "day")) %>%
  mutate(year = as.numeric(year), month = as.numeric(month), 
         day = as.numeric(day)) %>% # make date into three separate columns 
  mutate(prcp = prcp * 0.0393701, snow = snow * 0.0393701, 
         snwd = snwd * 0.0393701, tmax = tmax / 10, 
         tmin = tmin / 10) # convert data into inches 
weather_data

ggplot(weather_data, aes(x = snow)) + 
  geom_histogram(na.rm = T, binwidth = 1) +
  coord_cartesian(xlim = c(0,25)) 
# The most commonly observed values are small numbers close to 0. This is because 
# when it snows, it only snows a little amount. 

```
```{r}

avg_tmax_df = weather_data %>%
  filter(month %in% c(1, 7), !is.na(tmax)) %>% # filter by month and exclude NA's 
  group_by(id, year, month) %>%  
  summarize(avg_tmax = mean(tmax)) # calculate averages 
avg_tmax_df  

ggplot(avg_tmax_df, aes(x = year, y = avg_tmax, color = id)) +
  geom_line() + theme(legend.position = "none") + facet_wrap(~month, nrow = 2)
```

```{r}
avg_temp_year = weather_data %>%
  filter(!is.na(tmax) & !is.na(tmin)) %>%
  group_by(year) %>%
  summarize(avg_tmax = mean(tmax), avg_tmin = mean(tmin)) %>%
  gather(key = variable, value = value, 2:3)
avg_temp_year

ggplot(avg_temp_year, aes(x = year, y = value)) +
  geom_line() + facet_wrap(~variable, nrow = 2, scales = "free_y") 

snowfall_df = weather_data %>%
  filter(snow > 0 & snow < 100) # filter for 0 < snow < 100
  
snowfall_df  

ggplot(snowfall_df, aes(x = snow)) +
  geom_histogram() + facet_wrap(~year) # separate graphs for each year 
```

