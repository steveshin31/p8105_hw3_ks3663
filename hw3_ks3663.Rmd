---
title: "p8105_hw3_ks3663"
author: "Kee-Young Shin"
date: "October 10, 2018"
output: github_document
---

## Problem 1
```{r}

library(tidyverse)
library(p8105.datasets)


data("brfss_smart2010") # import data

filtered_brfss = brfss_smart2010 %>%
  janitor::clean_names() %>%
  select(year, locationabbr, locationdesc, topic, response, sample_size, data_value) %>%
  filter(topic == "Overall Health") %>% # filter based on overall health
  mutate(response_f = factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"), ordered = TRUE)) # make column for factor variables 

levels(filtered_brfss$response_f)
class(filtered_brfss$response_f)
filtered_brfss

```
```{r}

filtered_brfss %>%
  filter(year == "2002") %>%
  select(locationabbr, locationdesc) %>%
  distinct() %>%
  group_by(locationabbr) %>%
  summarize(count = n()) %>%
  filter(count == 7)
# CT, FL, and NC were observed at 7 locations

locations_count = filtered_brfss %>%
  select(year, locationabbr, locationdesc) %>%
  distinct() %>% # distinct states
  group_by(year, locationabbr) %>%
  summarize(count = n()) %>% # count number of locations
  arrange(-count)
locations_count

ggplot(locations_count, aes(x = year, y = count, color = locationabbr)) + 
  geom_line()


```
As can be seen by the data, most of the states were observed at fewer than 10 locations. Some outliers do exist: Florida had 44 and 41 locations observed in 2007 and 2010, respectively, which is much higher than the other states.

```{r}

filtered_brfss %>%
  select(locationabbr, locationdesc, year, data_value, response) %>%
  filter(locationabbr == "NY", year %in% c(2002, 2006, 2010), 
         response == "Excellent") %>%
  group_by(year) %>%
  summarize(mean = mean(data_value), std = sd(data_value)) # show mean and std

```
The averag proportion of excellent responses in NY State was highest in 2002 with around 24%. The average proportions were similar in 2006 and 2010. The standard deviation of the proportions was also highest in 2002. 
```{r}

average_proportions = filtered_brfss %>%
  select(locationabbr, locationdesc, year, response_f, sample_size) %>%
  spread(response_f, value = sample_size) %>%
  mutate(total_sample = (Excellent + `Very good` + Good + Fair + Poor)) %>%
  mutate(proportion_ex = Excellent / total_sample, proportion_vg = `Very good` / total_sample, proportion_g = Good / total_sample, proportion_f = Fair / total_sample, proportion_p = Poor / total_sample) %>% # create column for averages
  group_by(year, locationabbr) %>% # group by year and location
  summarize(Excellent = mean(proportion_ex), `Very good` = mean(proportion_vg), Good = mean(proportion_g), Fair = mean(proportion_f), Poor = mean(proportion_p)) %>%
  ungroup() %>%
  gather(key = variable, value = value, 3:7) %>% # reorder dataset 
  mutate(factor_variable = factor(variable, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"), ordered = TRUE)) %>% 
  arrange(factor_variable)

  



ggplot(average_proportions, aes(x = year, y = value, group = year)) + geom_boxplot() + facet_wrap(~variable) 
```

## Problem 2
```{r}

data("instacart") # import data

instacart

```
There are `r nrow(distinct(instacart, order_id))` orders in this dataset and a total of `r nrow(distinct(instacart, product_id))` products ordered. The average hour of the day at which people buy the products is `r mean(instacart$order_hour_of_day)`, which shows many people shot during the evening hours. 
```{r}

distinct(instacart, aisle_id) 
# There are 134 different aisles. 

aisle_orders = instacart %>%
  group_by(aisle) %>%
  summarize(n = n()) %>%
  arrange(-n)
# The most items are ordered from fresh vegetables and fresh fruits. 

aisle_orders

aisle_orders$aisle = factor(aisle_orders$aisle, levels =
                              aisle_orders$aisle[order(aisle_orders$n)]) 

ggplot(aisle_orders, aes(x = aisle, y = n)) + 
  geom_point() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) # make plot of counts and put it in sensible order

```
The plot shows that the number of items ordered from the aisles are mostly less than 25000. There are some outliers such as the two aisles that had above 150,000 orders (fresh fruits and fresh vegetables). 
```{r}

instacart %>%
  filter(aisle %in% c("dog food care", "baking ingredients", "packaged vegetables fruits")) %>% # filter by selected aisles 
  group_by(product_name, aisle) %>%
  summarize(count = n()) %>% # take count 
  ungroup() %>%
  group_by(aisle) %>%
  arrange(desc(count)) %>% # arrange count in descending order 
  slice(1) # show most ordered item 

```
Light brown sugar, snack sticks chicken dog treats, and organic baby spinach were the three most popular items for aisles baking ingredients, dog food care, and packaged vegetables fruits, respectively. The count for the orders is the smallest for the dog treats, which would make sense because the population of people that would buy these treats is generally smaller than those that buy general food products.
```{r}

instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>% # calculate mean 
  spread(order_dow, mean_hour) # spread days into columns 


```
The average time at which people buy these products seem to be around earlier in the day to the evening. 

## Problem 3
```{r}
data("ny_noaa")
ny_noaa
dim(ny_noaa)
summary(ny_noaa$tmax)

  
```
The data set is quite large with over 2 million observations. However there are a lot of missing values which is due to the fact that many stations only report precipitation. 
```{r}
weather_data = ny_noaa %>%
  janitor::clean_names() %>%
  mutate(tmax = as.numeric(tmax), tmin = as.numeric(tmin)) %>%
  separate(date, into = c("year", "month", "day")) %>%
  mutate(year = as.numeric(year), month = as.numeric(month), 
         day = as.numeric(day)) %>% # make date into three separate columns 
  mutate(prcp = (prcp / 10) * 0.0393701, snow = snow * 0.0393701, 
         snwd = snwd * 0.0393701, tmax = tmax / 10, 
         tmin = tmin / 10) # convert data into inches 
weather_data

weather_data %>%
  group_by(snow) %>% 
  summarize(count = n()) %>%
  arrange(desc(count))
  
```
The most commonly observed value is 0. This makes sense because it does not snow majority of the year therefore there are many recorded 0 inch values. There are many NA's since many stations do not record snowfall. 
```{r}

avg_tmax_df = weather_data %>%
  filter(month %in% c(1, 7), !is.na(tmax)) %>% # filter by month and exclude NA's 
  group_by(id, year, month) %>%  
  summarize(avg_tmax = mean(tmax)) # calculate averages 
avg_tmax_df  

ggplot(avg_tmax_df, aes(x = year, y = avg_tmax, color = id)) +
  geom_line() + theme(legend.position = "none") + facet_wrap(~month, nrow = 2)
```
As is expected, the average max temperature across the years is much lower during January than it is during July. The average tmax during January seem more erratic with more up and down spikes than that of July. Furthermore, when looking at January plot, there appears to be an overall upward trend, though very slight; more average tmax values can be found to be above 0 degrees Celsius in the later years. In terms of outliers, there is one location that experienced a much lower average tmax (about 13 degrees Celsius) in July around 1987. 
```{r}
avg_temp_year = weather_data %>%
  filter(!is.na(tmax) & !is.na(tmin)) %>%
  group_by(year) %>%
  summarize(avg_tmax = mean(tmax), avg_tmin = mean(tmin)) %>%
  gather(key = variable, value = value, 2:3)
avg_temp_year


ggplot(avg_temp_year, aes(x = year, y = value)) +
  geom_line() + facet_wrap(~variable, nrow = 2, scales = "free_y") 

avg_temp_year = weather_data %>%
  filter(!is.na(tmax) & !is.na(tmin)) %>%
  group_by(year)
avg_temp_year

ggplot(avg_temp_year, aes(x = tmax, y = tmin)) + geom_hex()


```
When looking at the distribtion of tmax and tmin values, we can see there is a general pattern: higher tmax values is usually associated with higher tmin values.
```{r}
snowfall_df = weather_data %>%
  filter(snow > 0 & snow < 100) # filter for 0 < snow < 100 inches
  
snowfall_df  

ggplot(snowfall_df, aes(x = snow)) +
  geom_histogram() + facet_wrap(~year) # separate graphs for each year 

```

It can be seen that the number of snow occurrences steadily increased over the years. The amount of snow in inches however seems to have stayed about the same through the years. 
